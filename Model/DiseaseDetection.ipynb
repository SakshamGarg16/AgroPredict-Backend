{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Found 17572 images belonging to 38 classes.\n",
      "Extracting features using ResNet50...\n",
      "138/138 [==============================] - 485s 4s/step\n"
     ]
    }
   ],
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Data generator to preprocess input images\n",
    "datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    r'S:\\Datasets\\Datasets\\Animal Plant Dataset\\New Plant Diseases Dataset(Augmented)\\Test',\n",
    "    target_size=(128, 128),  # Full resolution\n",
    "    batch_size=128,           # Optimal batch size for GPU\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Extract features using ResNet50\n",
    "print(\"Extracting features using ResNet50...\")\n",
    "train_features = base_model.predict(train_gen, verbose=1)\n",
    "train_labels = train_gen.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing dimensionality with PCA...\n",
      "Training SVM...\n",
      "SVM training complete!\n"
     ]
    }
   ],
   "source": [
    "# Flatten features for PCA and SVM\n",
    "train_features_flattened = train_features.reshape(train_features.shape[0], -1)\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "print(\"Reducing dimensionality with PCA...\")\n",
    "pca = PCA(n_components=512)  # Reduced components to avoid memory issues\n",
    "train_features_reduced = pca.fit_transform(train_features_flattened)\n",
    "\n",
    "# Train SVM (on CPU)\n",
    "print(\"Training SVM...\")\n",
    "svm_model = make_pipeline(StandardScaler(), SVC(kernel='linear', probability=True))\n",
    "svm_model.fit(train_features_reduced, train_labels)\n",
    "print(\"SVM training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA model saved as 'pca_model.pkl'\n",
      "SVM model saved as 'svm_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Save PCA model\n",
    "joblib.dump(pca, 'pca_model.pkl')\n",
    "print(\"PCA model saved as 'pca_model.pkl'\")\n",
    "\n",
    "# Save SVM model\n",
    "joblib.dump(svm_model, 'svm_model.pkl')\n",
    "print(\"SVM model saved as 'svm_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save class indices\n",
    "import json\n",
    "with open('class_indices.json', 'w') as f:\n",
    "    json.dump(train_gen.class_indices, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
